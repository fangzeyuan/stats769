---
title: "Lecture (Week 12b): Neural Networks and Deep Learning"
output: html_document
---

```{r echo=FALSE}
knitr::opts_chunk$set(
  comment = ''
)
```

# 1. Data

```{r, fig.dim=c(10,8)}
data(iris)
attach(iris)
plot(iris[,1:4], col=as.numeric(Species)+1, main="Iris Data")
```

Load libraries:

```{r}
library(neuralnet)           # class nn, with plot.nn()
library(NeuralNetTools)      # plotnet()
```

# 2. Single Layer Neural Networks

```{r, eval=TRUE, fig.dim=c(10,8)}
formula = Species ~ .
r = neuralnet(formula, iris, hidden=5, rep=1, err.fct='ce', linear.output=FALSE)   # minimise the cross-entropy
plotnet(r, cex=0.7)
head(p <- predict(r, iris[,1:4]))
mean(apply(p, 1, which.max) == as.numeric(Species))      # training accuracy
```

# 3. Multiple Layer Neural Networks

```{r, eval=TRUE, fig.dim=c(10,8)}
r = neuralnet(formula, iris, hidden=c(5, 3, 6), rep=1, err.fct='ce', linear.output=FALSE)
plotnet(r, rep="best", cex=0.7)
head(p <- predict(r, iris[,1:4]))
mean(apply(p, 1, which.max) == as.numeric(Species))      # training accuracy
detach(iris)
```

# 4. A Taste of Deep Learning

(Code modified from Lab 19.9.3, ISLv2)

[It may take quite a while to successfully install the R package
`keras` on your own computer, including the installation of a couple
of not-R packages (Miniconda and tensorflow). Code below works fine on
my own computer, but not yet on VMs which have no internet access for
a successful installation of these packages.]

```{r eval=TRUE}
library(keras)
```

```{r eval=TRUE}
cifar100 = dataset_cifar100()     # will ask to install Miniconda (and Python?)
names(cifar100)
train.x = cifar100$train$x / 255
dim(train.x)
train.y = drop(cifar100$train$y)
length(train.y)
table(train.y)
test.x = cifar100$test$x / 255
test.y = drop(cifar100$test$y)
table(test.y)
train.ymat = to_categorical(train.y, 100)
dim(train.ymat)
```

```{r eval=TRUE, fig.dim=c(10,10)}
library(jpeg)
par(mar=rep(0,4), mfrow=c(10,10))
o = order(train.y)
index = o[0:99 * 500 + sample(500, 100)]        # one image for each class
for(i in index) plot(as.raster(train.x[i,,,]))
```

```{r eval=TRUE}
m = 5                # choose m=5 arbitrary classes out of 100
(classes = sort(sample(100, m)) - 1)       # starting from class 0
new = integer(100)
new[classes] = 1:m - 1                     # new class labels, starting from class 0

# training set
i = train.y %in% classes
train1.x = train.x[i,,,]
dim(train1.x)
train1.y = new[train.y[i]]
length(train1.y)
sample(train1.y, 20)
train1.ymat = to_categorical(train1.y, m)
dim(train1.ymat)

# test set
i2 = test.y %in% classes
test1.x = test.x[i2,,,]
dim(test1.x)
test1.y = new[test.y[i2]]
length(test1.y)
sample(test1.y, 20)
```

```{r eval=TRUE, fig.dim=c(10,5)} 
par(mar=rep(0,4), mfrow=c(m,10))
for(j in 1:m) {
  k = sample(which(train.y == classes[j]), 10)
  for(i in k) plot(as.raster(train.x[i,,,]))
}
```

The pipe operator "%>%" (provided by `keras`) passes the value on the
left-hand side to the function on the right-hand side as its first
argument.

```{r eval=TRUE}
1:10 %>% pmax(5)           # same as pmax(1:10, 5)
```

```{r eval=TRUE, fig.dim=c(10,8)}
model =                    # same model as used on page 449, ISLv2, for m=5 classes here
  keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(32,32,3)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=512, activation="relu") %>%
  layer_dense(units=m, activation="softmax")

summary(model)

model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))

n1 = m*500                      # training size
i = sample(m*500, n1)      
system.time(history <- model %>% fit(train1.x[i,,,], train1.ymat[i,], epochs=30, batch_size=128, validation_split=0.2))
plot(history)
```

`epoch` is the number of times the algorithm passes through the
training set.

```{r}
n2 = m*100                      # test size
i2 = sample(m*100, n2)
yhat = model %>% predict(test1.x[i2,,,]) %>% k_argmax() %>% as.integer()
mean(yhat == test1.y[i2])       # test accuracy
```

This is much better than random guessing which has an expected
accuracy of 1/m.

With so much data and so many parameters, computation is very memory-demanding.

<hr/>
