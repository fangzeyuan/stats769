accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
})
})
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call(rbind,nc,deparse.level=0)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call(rbind(deparse.level = 0),nc)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
do.call
?do.call
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call(rbind(deparse.level = 0),nc,quote=T)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call("rbind(deparse.level = 0)",nc,quote=T)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
print(cv_res)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
r?bind
?rbind
res
a = list(c(1,2),c(2,3))
a
a[-2]
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
print(nc)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
vc = read.csv("vertebral-column.csv", stringsAsFactors=TRUE)
head(vc)
library(MASS)
library(class)
library(gam)
library(nnet)
library(e1071)
library(parallel)
#plot(vc,col=vc$class)
plot(vc[,-7], col=as.numeric(vc[,7])+1, pch=1, main="Vertebral Column Data")
(r1 = lda(class ~. , data=vc))
p1 = predict(r1, newdata=vc)
yhat1 = p1$class
table(vc$class, yhat1)
(A1 = mean(vc$class == yhat1))
(r2 = qda(class ~ ., data=vc))
(yhat2 = predict(r2, newdata=vc)$class)
table(vc$class, yhat2)
(A2=mean(vc$class==yhat2))
(r3 = naiveBayes(class ~ ., data=vc))
(yhat3 = predict(r3, newdata=vc))
table(vc$class, yhat3)
(A3=mean(vc$class==yhat3))
(r4 = multinom(class ~ ., data=vc))
(yhat4 <- predict(r4,vc))
table(vc$class, yhat4)
(A4= mean(vc$class==yhat4))
set.seed(769)
(yhat5 = knn(train=vc[,1:6], test=vc[,1:6], cl=vc[,7], k=10))   # K = 10
table(vc[,7], yhat5)
(A5= mean(vc[,7]==yhat5))
res <- c("LDA"=A1,"QDA"=A2,"NB"=A3,"MultLogReg"=A4,"KNN"=A5)
sort(res)
(vc$bi_class <- factor(ifelse(vc$class=="SL" | vc$class=="DH","AB","NO")))
(r6 = glm(bi_class ~.-class, data=vc, family=binomial))
p6<-predict(r6,vc,type="response")
yhat6 <-as.numeric(p6>0.5)+1
table(vc$bi_class,levels(vc$bi_class)[yhat6])
mean(vc$bi_class == levels(vc$bi_class)[yhat6])
r7 <- step(r6,type="backward")
summary(r7)
r8 <- gam(bi_class ~ s(pi,5) + s(pt,5) + s(pr,5) + s(gos,5) ,data=vc, family=binomial())
plot(r8)
r9 <- gam(bi_class ~ pi + pt + s(pr,2) + gos ,data=vc, family=binomial())
p9 <- predict(r9,vc,type = "response")
yhat9 <-as.numeric(p9>0.5)+1
table(vc$bi_class,levels(vc$bi_class)[yhat9])
(A9= mean(vc$bi_class ==levels(vc$bi_class)[yhat9]))
vc = vc[,-8]
n=nrow(vc)
R = 10                   # number of repetitions
K = 10
accuracy = matrix(nrow=R*K, ncol=5)
test.set = function(i, n, K=10) {
if(i < 1 || i > K)
stop(" i out of range (1, K)")
start = round(1 + (i-1) * n / K)
end = ifelse(i == K, n, round(i * n / K))
start:end
}
set.seed(769)
for(i in 1:R) {                  # for each repetition
ind = sample(n)
for(k in 1:K) {                # for each fold
index = ind[test.set(k, n, K)]
test = vc[index,]
train = vc[-index,]
r1 = lda(class ~. , data=train)
accuracy[K*(i-1)+k,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[K*(i-1)+k,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[K*(i-1)+k,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace= F)
accuracy[K*(i-1)+k,4] = mean(test$class == predict(r4,newdata = test))
accuracy[K*(i-1)+k,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
}
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(accuracy) )
cv_res[order(-cv_res$accuracy),]
n=nrow(vc)
R = 10                   # number of repetitions
K = 10
test.set = function(i, n, K=10) {
if(i < 1 || i > K)
stop(" i out of range (1, K)")
start = round(1 + (i-1) * n / K)
end = ifelse(i == K, n, round(i * n / K))
start:end
}
set.seed(769)
cv <- list()
for(i in 1:K) {                  # for each repetition
cv[[K]] <- list()
ind = sample(n)
for(k in 1:R) {                # for each fold
index = ind[test.set(k, n, K)]
cv[[i]][[k]] <- index
}
}
lapply(c(1,5,10,20),function(nc){
system.time({
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
})
})
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
print(nc)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
nc <- nc[-1,-2]
print(nc)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
nc <- nc[c(-1,-2)]
print(nc)
# cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
# print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
nc <- nc[c(-1,-2)]
print(nc)
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
nc <- nc[c(-1,-2)]
#print(nc)
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#
nc <- nc[c(-1,-2)]
cv_res <- do.call(rbind,c(nc,list=(deparse.level = 0)))
#print(nc)
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
print(cv_res[order(-cv_res$accuracy),])
}
res <- lapply(c(1,5,10,20),function(nc){
cv_res = c(cv_res, mclapply(1:10,function(k){
accuracy = matrix(nrow=R,ncol = 5)
for(i in 1:R){
test = vc[cv[[i]][[k]],]
train =vc[-cv[[i]][[k]],]
r1 = lda(class ~. , data=train)
accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
r2 = qda(class ~ ., data=train)
accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
r3 = naiveBayes(class ~ ., data=train)
accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
r4 = multinom(class ~ ., data=train,trace=F)
accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
}
return(accuracy)
},mc.cores = nc))
return(cv_res)
})
for(nc in res ){
#
nc <- nc[c(-1,-2)]
cv_res <- do.call(rbind,nc)
#print(nc)
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
print(cv_res[order(-cv_res$accuracy),])
}
(yhat5 = knn(train=vc[,1:6], test=vc[,1:6], cl=vc[,7], k=10))   # K = 10
table(vc[,7], yhat5)
(A5= mean(vc[,7]==yhat5))
res <- c("LDA"=A1,"QDA"=A2,"NB"=A3,"MultLogReg"=A4,"KNN"=A5)
sort(res)
