---
title: "lab8"
author: "Zeyuan Fang"
date: "9/27/2021"
output: html_document
---

upi: zfan253\
id: 556588804\

## Data description

The data source for this lab is from UCI Machine Learning Repository. We are going to use the Vertebral Column data set, the description of which can be found here: https://archive.ics.uci.edu/ml/datasets/Vertebral+Column.\
The data set has 6 numeric predictors: pi (pelvic incidence), pt (pelvic tilt), lla (lumbar lordosis angle), ss (sacral slope), pr (pelvic radius), gos (grade of spondylolisthesis). The response variable is class, which has three levels: DH (Disk Hernia), SL (Spondylolisthesis), NO (Normal).\

Read in data.

```{r}
vc = read.csv("vertebral-column.csv", stringsAsFactors=TRUE)
head(vc)
```

To perform the following tasks, we need load the following libraries:

```{r}
library(MASS)
library(class)
library(gam)
library(nnet)
library(e1071)
library(parallel)
```

## Exploration

#### Q1. Create a pairwise scatterplot, with observations of different classes shown in different colors.


```{r }
#plot(vc,col=vc$class)
plot(vc[,-7], col=as.numeric(vc[,7])+1, pch=1, main="Vertebral Column Data")
```
## Classification

Five classification methods are listed in the next 5 tasks. Use each of them to build a model for the data set vc and predict the class labels of the observations that are used to build the model. For each, compute the confusion matrix and (resubstitution) classification accuracy.

#### Q2. Linear discriminant analysis.


```{r}
(r1 = lda(class ~. , data=vc))
```

```{r}
p1 = predict(r1, newdata=vc)
yhat1 = p1$class
table(vc$class, yhat1)
```

```{r}
(A1 = mean(vc$class == yhat1))
```

#### Q3. Quadratic discriminant analysis.

```{r}
(r2 = qda(class ~ ., data=vc))
```

```{r}
(yhat2 = predict(r2, newdata=vc)$class)
```

```{r}
table(vc$class, yhat2)  
```

```{r}
(A2=mean(vc$class==yhat2))
```

#### Q4. Naive Bayes.

```{r}
(r3 = naiveBayes(class ~ ., data=vc))
```

```{r}
(yhat3 = predict(r3, newdata=vc))
```

```{r}
table(vc$class, yhat3) 
```

```{r}
(A3=mean(vc$class==yhat3))
```

#### Q5. Multinomial logistic regression.

```{r}
(r4 = multinom(class ~ ., data=vc))
```

```{r}
(yhat4 <- predict(r4,vc))
```

```{r}
table(vc$class, yhat4)  
```

```{r}
(A4= mean(vc$class==yhat4))
```

#### Q6. K-nearest neighbours (with K=10).

```{r}
set.seed(769)
(yhat5 = knn(train=vc[,1:6], test=vc[,1:6], cl=vc[,7], k=10))   # K = 10
```

```{r}
table(vc[,7], yhat5)
```

```{r}
(A5= mean(vc[,7]==yhat5))
```

## Primary Performance Evaluation

#### Q7. Present your resulting classification accuracy for all five classification methods in a table (in whatever format). From this table, what can we say about the relative performance of these methods for the data set?

```{r}
res <- c("LDA"=A1,"QDA"=A2,"NB"=A3,"MultLogReg"=A4,"KNN"=A5)
sort(res)
```


## Multiple Logistic Regression and Generalised Additive Models

#### Q8. Create a response variable from variable class so that both classes DH and SL are relabelled as AB (Abnormal). Use glm() to build a multiple logistic regression model for this new class variable, and compute the confusion matrix and resubstitution classification accuracy.

```{r}
(vc$bi_class <- factor(ifelse(vc$class=="SL" | vc$class=="DH","AB","NO")))
```

```{r}
(r6 = glm(bi_class ~.-class, data=vc, family=binomial))
```


```{r}
p6<-predict(r6,vc,type="response")
yhat6 <-as.numeric(p6>0.5)+1
table(vc$bi_class,levels(vc$bi_class)[yhat6])
```

```{r}
mean(vc$bi_class == levels(vc$bi_class)[yhat6])  
```

The accuracy does not improve though the response variable now only has two levels.( from 87% to 85%)

#### Q9. Use step() to find the AIC-selected model, with backward selection. Which variables are removed by the AIC?

```{r,warning=F}
r7 <- step(r6,type="backward")
```
```{r}
summary(r7)
```

lla, ss are removed by the AIC.

#### Q10. Extend the AIC-selected model with gam() so that each linear term is replaced with a smoothing spline of 5 degrees of freedom. Take a visual approach to reasonably lower the degrees of freedom in each term. For your chosen model, compute the confusion matrix and classification accuracy.

```{r}
r8 <- gam(bi_class ~ s(pi,5) + s(pt,5) + s(pr,5) + s(gos,5) ,data=vc, family=binomial())
```
```{r}
plot(r8)
```
```{r}
r9 <- gam(bi_class ~ pi + pt + s(pr,2) + gos ,data=vc, family=binomial())
```

```{r}
p9 <- predict(r9,vc,type = "response")
yhat9 <-as.numeric(p9>0.5)+1
table(vc$bi_class,levels(vc$bi_class)[yhat9])
```

```{r}
(A9= mean(vc$bi_class ==levels(vc$bi_class)[yhat9]))
```

## Cross-validation and Parallel Computing

#### Q11. Reconsider the 3-class problem studied in Questions 2-7. Use 10 repetitions of 10-fold cross-validation to evaluate the performance of the 5 classification methods.Present your resulting classification accuracy of all five classification methods in a table. Comment on the results.

remove 2-class label.

```{r}
vc = vc[,-8]
```


```{r}
n=nrow(vc)

R = 10                   # number of repetitions
K = 10   
accuracy = matrix(nrow=R*K, ncol=5)
test.set = function(i, n, K=10) {
  if(i < 1 || i > K) 
    stop(" i out of range (1, K)")
  start = round(1 + (i-1) * n / K)
  end = ifelse(i == K, n, round(i * n / K))
  start:end
}

set.seed(769)  
for(i in 1:R) {                  # for each repetition
  ind = sample(n)
  for(k in 1:K) {                # for each fold
    index = ind[test.set(k, n, K)]
    test = vc[index,]
    train = vc[-index,]
    r1 = lda(class ~. , data=train)
    accuracy[K*(i-1)+k,1] = mean(test$class ==predict(r1,newdata = test)$class)
    r2 = qda(class ~ ., data=train)
    accuracy[K*(i-1)+k,2] = mean(test$class ==predict(r2,newdata = test)$class)
    r3 = naiveBayes(class ~ ., data=train)
    accuracy[K*(i-1)+k,3] = mean(test$class==predict(r3,newdata = test))
    r4 = multinom(class ~ ., data=train,trace= F)
    accuracy[K*(i-1)+k,4] = mean(test$class == predict(r4,newdata = test))
    accuracy[K*(i-1)+k,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
    
  }
}

```
```{r}
cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(accuracy) )
cv_res[order(-cv_res$accuracy),]
```

#### Q12. Modify your code so that parallel computing can be used, in which each job running in parallel is only for the computation about one fold out of 10 (i.e., one training set and one test set). Make sure that the same subsamples are used by different methods and that the results are reproducible. Compare timings with 1, 5, 10, 20 cores

```{r}
n=nrow(vc)
R = 10                   # number of repetitions
K = 10   

test.set = function(i, n, K=10) {
  if(i < 1 || i > K) 
    stop(" i out of range (1, K)")
  start = round(1 + (i-1) * n / K)
  end = ifelse(i == K, n, round(i * n / K))
  start:end
}
set.seed(769)  
cv <- list()
for(i in 1:K) {                  # for each repetition
    cv[[K]] <- list()
    ind = sample(n)
    for(k in 1:R) {                # for each fold
      index = ind[test.set(k, n, K)]
      cv[[i]][[k]] <- index
  }
}

```

```{r}
lapply(c(1,5,10,20),function(nc){
  system.time({
  cv_res = c(cv_res, mclapply(1:10,function(k){
    accuracy = matrix(nrow=R,ncol = 5)
    for(i in 1:R){
      test = vc[cv[[i]][[k]],]
      train =vc[-cv[[i]][[k]],]
      r1 = lda(class ~. , data=train)
      accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
      r2 = qda(class ~ ., data=train)
      accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
      r3 = naiveBayes(class ~ ., data=train)
      accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
      r4 = multinom(class ~ ., data=train,trace=F)
      accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
      accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
    }
    return(accuracy)
  },mc.cores = nc))
  })
})

```

Reproducible result

```{r}
res <- lapply(c(1,5,10,20),function(nc){
  cv_res = c(cv_res, mclapply(1:10,function(k){
    accuracy = data.frame(matrix(nrow=R,ncol = 5))
    for(i in 1:R){
      test = vc[cv[[i]][[k]],]
      train =vc[-cv[[i]][[k]],]
      r1 = lda(class ~. , data=train)
      accuracy[i,1] = mean(test$class ==predict(r1,newdata = test)$class)
      r2 = qda(class ~ ., data=train)
      accuracy[i,2] = mean(test$class ==predict(r2,newdata = test)$class)
      r3 = naiveBayes(class ~ ., data=train)
      accuracy[i,3] = mean(test$class==predict(r3,newdata = test))
      r4 = multinom(class ~ ., data=train,trace=F)
      accuracy[i,4] = mean(test$class == predict(r4,newdata = test))
      accuracy[i,5] =mean(test$class ==knn(train=train[,1:6], test=test[,1:6], cl=train[,7], k=10))
    }
    return(accuracy)
  },mc.cores = nc))
  return(cv_res)
})
for(nc in res ){
  nc <- nc[c(-1,-2)]
  cv_res <- do.call(rbind.data.frame,nc)
  #cv_res <- as.data.frame(sapply(cv_res[,1:5],as.numeric))
  #cv_res <- cv_res[complete.cases(cv_res),]
  #print(nc)
  
  cv_res <-data.frame( classifier=  c("LDA","QDA","NB","MultLogReg","KNN"), accuracy= colMeans(cv_res) )
  print(cv_res[order(-cv_res$accuracy),])
}


```

## Summary 

In this lab, we explore classification problems. First, we use LDA, QDA, Naive Bayes, Multinomial logistic regression ,K-nearest neighbours method respectively on the data set, we find the performance order is  KNN> MLR= QDA > NB >LDA. The target variable of the data set has three levels, then we consider to combine two of them to make it two levels. Then we build MLR and gam classifier for this two-level target variable, and we fine-tune the gam model based on a visual approach. Finally to get a more convincing evaluation for model performance of the 3-class problem studied in Questions 2-7, we use 10 repetitions of 10-fold cross-validation to evaluate the performance of the 5 classification methods and compute it by parallel, and we find MLR> QDA>NB>KNN>LDA. By CV, we could say MLR and QDA are better classifiers than other methods.
